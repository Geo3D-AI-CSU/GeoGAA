from torch_geometric.nn import SAGEConv, GATConv
from torch_geometric.utils import subgraph
import torch
import torch.nn as nn
import torch.nn.functional as F


# ============================================================================
# ÊñπÊ°à1: Âü∫Á°Ä‰∏âÂ±ÇÊâ©Â±ïÁâàÊú¨
# ============================================================================
class GATSAGEMultiTaskPredictor_V1(nn.Module):
    """
    ÊîπËøõÁâàÊú¨1: Âü∫Á°Ä‰∏âÂ±ÇÊâ©Â±ï

    Êû∂ÊûÑËØ¥Êòé:
    - GAT: 3Â±ÇÂõæÊ≥®ÊÑèÂäõÁΩëÁªúÔºåÊçïËé∑Â±ÄÈÉ®Ê≥®ÊÑèÂäõÊ®°Âºè
    - GraphSAGE: 3Â±ÇÈÇªÂüüËÅöÂêàÁΩëÁªúÔºåËÅöÂêàÈÇªÂüü‰ø°ÊÅØ
    - ÁâπÂæÅËûçÂêà: ÁÆÄÂçïÊãºÊé• + Á∫øÊÄßÂèòÊç¢
    - ‰ªªÂä°Â§¥: ÂàÜÂà´Áî®‰∫éLevelÈ¢ÑÊµã(ÂõûÂΩí)ÂíåRockÂàÜÁ±ª(ÂàÜÁ±ª)

    ÊîπËøõÁÇπ:
    1. Â∞ÜGATÂíåGraphSAGEÈÉΩÊâ©Â±ïÂà∞3Â±Ç
    2. ÊØèÂ±ÇÂêéÊ∑ªÂä†LayerNormÁ®≥ÂÆöËÆ≠ÁªÉ
    3. GATÊúÄÂêé‰∏ÄÂ±Ç‰ΩøÁî®Âπ≥ÂùáËÅöÂêà(concat=False)
    4. ÈÄÇÂΩìÁöÑDropoutÈò≤Ê≠¢ËøáÊãüÂêà

    ÂèÇÊï∞:
    - in_channels: ËæìÂÖ•ÁâπÂæÅÁª¥Â∫¶ (3 + Êñ≠Â±ÇÁâπÂæÅÊï∞)
    - hidden_channels: ÈöêËóèÂ±ÇÁª¥Â∫¶
    - gat_heads: GATÊ≥®ÊÑèÂäõÂ§¥Êï∞
    - num_classes: Â≤©ÊÄßÂàÜÁ±ªÁ±ªÂà´Êï∞
    - dropout: DropoutÊ¶ÇÁéá
    - activation_fn: ÊøÄÊ¥ªÂáΩÊï∞Á±ªÂûã ('prelu' or 'softplus')
    """

    def __init__(self, in_channels, hidden_channels=128, gat_heads=2,
                 num_classes=13, dropout=0.0, activation_fn='prelu'):
        super(GATSAGEMultiTaskPredictor_V1, self).__init__()

        self.dropout = dropout
        self.gat_heads = gat_heads

        print(f"\n{'=' * 80}")
        print(f"üèóÔ∏è  ÂàùÂßãÂåñ ")
        print(f"{'=' * 80}")
        print(f"  ËæìÂÖ•Áª¥Â∫¶: {in_channels}")
        print(f"  ÈöêËóèÁª¥Â∫¶: {hidden_channels}")
        print(f"  GATÊ≥®ÊÑèÂäõÂ§¥Êï∞: {gat_heads}")
        print(f"  Â≤©ÊÄßÁ±ªÂà´Êï∞: {num_classes}")
        print(f"  DropoutÊ¶ÇÁéá: {dropout}")
        print(f"  ÊøÄÊ¥ªÂáΩÊï∞: {activation_fn}")
        print(f"{'=' * 80}\n")

        # ========== GATÁâπÂæÅÊèêÂèñ (3Â±Ç) ==========
        # Á¨¨1Â±Ç: in_channels -> hidden_channels * gat_heads
        self.gat1 = GATConv(
            in_channels,
            hidden_channels,
            heads=gat_heads,
            dropout=dropout,
            concat=True  # ÊãºÊé•Â§öÂ§¥ËæìÂá∫
        )
        self.gat_norm1 = nn.LayerNorm(hidden_channels * gat_heads)

        # Á¨¨2Â±Ç: hidden_channels * gat_heads -> hidden_channels * gat_heads
        self.gat2 = GATConv(
            hidden_channels * gat_heads,
            hidden_channels,
            heads=gat_heads,
            dropout=dropout,
            concat=True
        )
        self.gat_norm2 = nn.LayerNorm(hidden_channels * gat_heads)

        # Á¨¨3Â±Ç: hidden_channels * gat_heads -> hidden_channels
        # Ê≥®ÊÑè: concat=False Ë°®Á§∫ÂØπÂ§öÂ§¥ËæìÂá∫ÂèñÂπ≥ÂùáËÄåÈùûÊãºÊé•
        self.gat3 = GATConv(
            hidden_channels * gat_heads,
            hidden_channels,
            heads=gat_heads,
            dropout=dropout,
            concat=False  # Âπ≥ÂùáËÅöÂêàÔºåËæìÂá∫Áª¥Â∫¶‰∏∫ hidden_channels
        )
        self.gat_norm3 = nn.LayerNorm(hidden_channels)

        # ========== GraphSAGEÁâπÂæÅÊèêÂèñ (3Â±Ç) ==========
        # Êé•Êî∂GATÁöÑËæìÂá∫ÔºåÁª¥Â∫¶‰∏∫ hidden_channels
        self.sage1 = SAGEConv(hidden_channels, hidden_channels, aggr='mean')
        self.sage_norm1 = nn.LayerNorm(hidden_channels)

        self.sage2 = SAGEConv(hidden_channels, hidden_channels, aggr='mean')
        self.sage_norm2 = nn.LayerNorm(hidden_channels)

        self.sage3 = SAGEConv(hidden_channels, hidden_channels, aggr='mean')
        self.sage_norm3 = nn.LayerNorm(hidden_channels)

        # ========== ÁâπÂæÅËûçÂêàÂ±Ç ==========
        # Â∞ÜGATÂíåSAGEÁöÑËæìÂá∫ÊãºÊé•ÂêéËûçÂêà
        self.fusion = nn.Linear(hidden_channels * 2, hidden_channels)
        self.fusion_norm = nn.LayerNorm(hidden_channels)

        # ========== MLP‰ªªÂä°Â§¥ ==========
        # LevelÈ¢ÑÊµãÂàÜÊîØ (ÂõûÂΩí‰ªªÂä°)
        self.level_mlp = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels // 2),
            nn.PReLU() if activation_fn == 'prelu' else nn.Softplus(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 2, hidden_channels // 4),
            nn.PReLU() if activation_fn == 'prelu' else nn.Softplus(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 4, 1)
        )

        # Â≤©ÊÄßÂàÜÁ±ªÂàÜÊîØ (ÂàÜÁ±ª‰ªªÂä°)
        self.rock_mlp = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels // 2),
            nn.PReLU() if activation_fn == 'prelu' else nn.Softplus(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 2, hidden_channels // 4),
            nn.PReLU() if activation_fn == 'prelu' else nn.Softplus(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 4, num_classes)
        )

        # ÊøÄÊ¥ªÂáΩÊï∞
        if activation_fn == 'softplus':
            self.activation = nn.Softplus()
        else:
            self.activation = nn.PReLU()

        # ÁªüËÆ°ÂèÇÊï∞Èáè
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"üìä Ê®°ÂûãÂèÇÊï∞ÁªüËÆ°:")
        print(f"  ÊÄªÂèÇÊï∞Èáè: {total_params:,}")
        print(f"  ÂèØËÆ≠ÁªÉÂèÇÊï∞: {trainable_params:,}")
        print(f"{'=' * 80}\n")

    def forward(self, x, edge_index):
        """
        ÂâçÂêë‰º†Êí≠

        ÂèÇÊï∞:
        - x: ËäÇÁÇπÁâπÂæÅ [num_nodes, in_channels]
        - edge_index: ËæπÁ¥¢Âºï [2, num_edges]

        ËøîÂõû:
        - level_output: LevelÈ¢ÑÊµãÂÄº [num_nodes]
        - rock_output: Â≤©ÊÄßÂàÜÁ±ªlogits [num_nodes, num_classes]
        """

        # ========== GATÁâπÂæÅÊèêÂèñ (3Â±Ç) ==========
        # Layer 1
        gat_out = self.gat1(x, edge_index)
        gat_out = self.gat_norm1(gat_out)
        gat_out = self.activation(gat_out)
        gat_out = F.dropout(gat_out, p=self.dropout, training=self.training)

        # Layer 2
        gat_out = self.gat2(gat_out, edge_index)
        gat_out = self.gat_norm2(gat_out)
        gat_out = self.activation(gat_out)
        gat_out = F.dropout(gat_out, p=self.dropout, training=self.training)

        # Layer 3 (Âπ≥ÂùáËÅöÂêà)
        gat_out = self.gat3(gat_out, edge_index)
        gat_out = self.gat_norm3(gat_out)
        gat_out = self.activation(gat_out)
        gat_out = F.dropout(gat_out, p=self.dropout, training=self.training)

        # ========== GraphSAGEÁâπÂæÅÊèêÂèñ (3Â±Ç) ==========
        # Layer 1
        sage_out = self.sage1(gat_out, edge_index)
        sage_out = self.sage_norm1(sage_out)
        sage_out = self.activation(sage_out)
        sage_out = F.dropout(sage_out, p=self.dropout, training=self.training)

        # Layer 2
        sage_out = self.sage2(sage_out, edge_index)
        sage_out = self.sage_norm2(sage_out)
        sage_out = self.activation(sage_out)
        sage_out = F.dropout(sage_out, p=self.dropout, training=self.training)

        # Layer 3
        sage_out = self.sage3(sage_out, edge_index)
        sage_out = self.sage_norm3(sage_out)
        sage_out = self.activation(sage_out)
        sage_out = F.dropout(sage_out, p=self.dropout, training=self.training)

        # ========== ÁâπÂæÅËûçÂêà ==========
        # ÊãºÊé•GATÂíåSAGEÁöÑËæìÂá∫
        fused_features = torch.cat([gat_out, sage_out], dim=-1)
        fused_features = self.fusion(fused_features)
        fused_features = self.fusion_norm(fused_features)
        fused_features = self.activation(fused_features)
        fused_features = F.dropout(fused_features, p=self.dropout, training=self.training)

        # ========== Â§ö‰ªªÂä°È¢ÑÊµã ==========
        # LevelÈ¢ÑÊµã (ÂõûÂΩí)
        level_output = self.level_mlp(fused_features).squeeze(-1)

        # Â≤©ÊÄßÂàÜÁ±ª
        rock_output = self.rock_mlp(fused_features)

        return level_output, rock_output


# ============================================================================
# ‰øùÁïôÂéüÊúâÊ®°Âûã‰ª•‰øùÊåÅÂÖºÂÆπÊÄß
# ============================================================================
class GATSAGEMultiTaskPredictor(nn.Module):
    """
    ÂéüÂßãÁâàÊú¨ (ÂÖºÂÆπÊÄß‰øùÁïô)
    GAT: 2Â±Ç + GraphSAGE: 2Â±Ç
    """

    def __init__(self, in_channels, hidden_channels=128, gat_heads=2,
                 num_classes=13, dropout=0.0, activation_fn='prelu'):
        super(GATSAGEMultiTaskPredictor, self).__init__()

        self.dropout = dropout
        self.gat_heads = gat_heads

        # ========== ÂõæÁâπÂæÅÊèêÂèñÂ±Ç ==========
        # GATÂ±Ç - ÊçïËé∑Â±ÄÈÉ®Ê≥®ÊÑèÂäõÊ®°Âºè
        self.gat1 = GATConv(in_channels, hidden_channels, heads=gat_heads, dropout=dropout)
        self.gat2 = GATConv(hidden_channels * gat_heads, hidden_channels, heads=gat_heads, dropout=dropout)

        # GraphSAGEÂ±Ç - ËÅöÂêàÈÇªÂüü‰ø°ÊÅØ
        self.sage1 = SAGEConv(hidden_channels * gat_heads, hidden_channels, aggr='mean')
        self.sage2 = SAGEConv(hidden_channels, hidden_channels, aggr='mean')

        # ÁâπÂæÅËûçÂêàÂ±Ç - ËûçÂêàGATÂíåSAGEÁöÑËæìÂá∫
        self.fusion = nn.Linear(hidden_channels * gat_heads + hidden_channels, hidden_channels)

        # ========== MLP‰ªªÂä°Â§¥ ==========
        # LevelÈ¢ÑÊµãÂàÜÊîØ (ÂõûÂΩí)
        self.level_mlp = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels // 2),
            nn.PReLU() if activation_fn == 'prelu' else nn.Softplus(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 2, hidden_channels // 4),
            nn.PReLU() if activation_fn == 'prelu' else nn.Softplus(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 4, 1)
        )

        # Â≤©ÊÄßÂàÜÁ±ªÂàÜÊîØ (ÂàÜÁ±ª)
        self.rock_mlp = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels // 2),
            nn.PReLU() if activation_fn == 'prelu' else nn.Softplus(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 2, hidden_channels // 4),
            nn.PReLU() if activation_fn == 'prelu' else nn.Softplus(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 4, num_classes)
        )

        # ÊøÄÊ¥ªÂáΩÊï∞
        if activation_fn == 'softplus':
            self.activation = nn.Softplus()
        else:
            self.activation = nn.PReLU()

    def forward(self, x, edge_index):
        # ========== GATÁâπÂæÅÊèêÂèñ ==========
        gat_out = self.gat1(x, edge_index)
        gat_out = self.activation(gat_out)
        gat_out = F.dropout(gat_out, p=self.dropout, training=self.training)

        gat_out = self.gat2(gat_out, edge_index)
        gat_out = self.activation(gat_out)
        gat_out = F.dropout(gat_out, p=self.dropout, training=self.training)

        # ========== GraphSAGEÁâπÂæÅÊèêÂèñ ==========
        sage_out = self.sage1(gat_out, edge_index)
        sage_out = self.activation(sage_out)
        sage_out = F.dropout(sage_out, p=self.dropout, training=self.training)

        sage_out = self.sage2(sage_out, edge_index)
        sage_out = self.activation(sage_out)
        sage_out = F.dropout(sage_out, p=self.dropout, training=self.training)

        # ========== ÁâπÂæÅËûçÂêà ==========
        fused_features = torch.cat([gat_out, sage_out], dim=-1)
        fused_features = self.fusion(fused_features)
        fused_features = self.activation(fused_features)
        fused_features = F.dropout(fused_features, p=self.dropout, training=self.training)

        # ========== Â§ö‰ªªÂä°È¢ÑÊµã ==========
        level_output = self.level_mlp(fused_features).squeeze(-1)
        rock_output = self.rock_mlp(fused_features)

        return level_output, rock_output


# ============================================================================
# ÂÖ∂‰ªñ‰øùÁïôÁöÑÊ®°ÂûãÁ±ª (‰øùÊåÅÂÖºÂÆπÊÄß)
# ============================================================================

class GATLevelPredictor(nn.Module):
    """GAT LevelÈ¢ÑÊµãÂô® (Âçï‰ªªÂä°)"""

    def __init__(self, in_channels, hidden_channels, heads=2, dropout=0, activation_fn='prelu'):
        super(GATLevelPredictor, self).__init__()

        self.heads = heads
        self.embed_dim = hidden_channels
        self.dropout = dropout

        self.conv1 = GATConv(in_channels, self.embed_dim, heads=heads, dropout=dropout)
        self.conv2 = GATConv(self.embed_dim * heads, self.embed_dim, heads=heads, dropout=dropout)
        self.conv3 = GATConv(
            self.embed_dim * heads,
            self.embed_dim,
            heads=heads,
            dropout=dropout,
            concat=False
        )

        self.level_predictor = nn.Linear(self.embed_dim, 1)

        if activation_fn == 'softplus':
            self.activation = nn.Softplus()
        else:
            self.activation = nn.PReLU()

    def forward(self, x, edge_index, edge_attr=None):
        x = self.conv1(x, edge_index)
        x = self.activation(x)
        x = F.dropout(x, p=self.dropout, training=self.training)

        x = self.conv2(x, edge_index)
        x = self.activation(x)
        x = F.dropout(x, p=self.dropout, training=self.training)

        x = self.conv3(x, edge_index)
        x = self.activation(x)
        x = F.dropout(x, p=self.dropout, training=self.training)

        level_output = self.level_predictor(x).squeeze(-1)
        return level_output


class LevelPredictor(torch.nn.Module):
    """GraphSAGE LevelÈ¢ÑÊµãÂô® (Âçï‰ªªÂä°)"""

    def __init__(self, in_channels, hidden_channels=128, out_channels=64,
                 activation_fn='prelu', dropout=0.0):
        super(LevelPredictor, self).__init__()

        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='mean')
        self.conv2 = SAGEConv(hidden_channels, hidden_channels, aggr='mean')
        self.conv3 = SAGEConv(hidden_channels, out_channels, aggr='mean')

        self.level_predictor = torch.nn.Linear(out_channels, 1)

        if activation_fn == 'softplus':
            self.activation = torch.nn.Softplus()
        else:
            self.activation = torch.nn.PReLU()

        self.dropout = dropout

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = self.activation(x)
        if self.dropout > 0:
            x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)

        x = self.conv2(x, edge_index)
        x = self.activation(x)
        if self.dropout > 0:
            x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)

        x = self.conv3(x, edge_index)
        level_output = self.level_predictor(x).squeeze(-1)

        return level_output


class RockUnitPredictor(torch.nn.Module):
    """GraphSAGE Â≤©ÊÄßÈ¢ÑÊµãÂô® (Âçï‰ªªÂä°)"""

    def __init__(self, in_channels, hidden_channels=128, out_channels=64,
                 num_classes=4, dropout=0.0):
        super(RockUnitPredictor, self).__init__()

        self.conv1 = SAGEConv(in_channels + 1, hidden_channels, aggr='mean')
        self.conv2 = SAGEConv(hidden_channels, hidden_channels, aggr='mean')
        self.conv3 = SAGEConv(hidden_channels, out_channels, aggr='mean')
        self.rock_unit_classifier = torch.nn.Linear(out_channels, num_classes)
        self.prelu = torch.nn.PReLU()

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = self.prelu(x)
        x = self.conv2(x, edge_index)
        x = self.prelu(x)
        x = self.conv3(x, edge_index)

        rock_unit_output = self.rock_unit_classifier(x)
        return rock_unit_output


class GATRockPredictor(nn.Module):
    """GAT Â≤©ÊÄßÈ¢ÑÊµãÂô® (Âçï‰ªªÂä°)"""

    def __init__(self, in_channels, hidden_channels, num_classes,
                 heads=2, dropout=0, activation_fn='prelu'):
        super(GATRockPredictor, self).__init__()

        self.heads = heads
        self.embed_dim = hidden_channels
        self.dropout = dropout

        self.conv1 = GATConv(in_channels, self.embed_dim, heads=heads, dropout=dropout)
        self.conv2 = GATConv(self.embed_dim * heads, self.embed_dim, heads=heads, dropout=dropout)
        self.conv3 = GATConv(
            self.embed_dim * heads,
            self.embed_dim,
            heads=heads,
            dropout=dropout,
            concat=False
        )

        self.rock_classifier = nn.Linear(self.embed_dim, num_classes)

        if activation_fn == 'softplus':
            self.activation = nn.Softplus()
        else:
            self.activation = nn.PReLU()

    def forward(self, x, edge_index, edge_attr=None):
        x = self.conv1(x, edge_index)
        x = self.activation(x)
        x = F.dropout(x, p=self.dropout, training=self.training)

        x = self.conv2(x, edge_index)
        x = self.activation(x)
        x = F.dropout(x, p=self.dropout, training=self.training)

        x = self.conv3(x, edge_index)
        x = self.activation(x)
        x = F.dropout(x, p=self.dropout, training=self.training)

        rock_unit_output = self.rock_classifier(x)
        return rock_unit_output